---
title: "Statistical Inference Course Project"
author: "Collin Mitchell"
date: "August 23, 2015"
output: html_document
---

# Overview
This paper was created to explore the Central Limit Theorem of Statistics.
The Central Limit Theorem states that as the number of samples increases, the sample mean of an Individual and Identically Distributed dataset will approach the Theoretical Mean of the Distribution. Let's show this is the case.

## Points to Prove
1. Show the sample mean and compare it to the theoretical mean of the distribution.
2. Show how variable the sample is (via variance) and compare it to the theoretical variance of the distribution.
3. Show that the distribution is approximately normal.



## Exploring Point [1] | The Sample Mean
We will be given:
```{r}
numberOfObservations    = 40
simulationCount         = 1000
lambda                  = .2

```
: where numberOfObeservations is how many data points we want, simulationCount is how many times we're going to run the experiment, and lambda is the rate at which the exponential function progresses.
       
With this information, we know that the Theoretical mean for our distribution should be 1/lambda. Or, in this case it will be 5. And, we also know that the Standard Deviation is the same: 1/lambda or 5.


```{r}
# Ensure no initial values to remove any bias.
means = NULL
# Run experiment
for(i in 1:simulationCount){
  if(i == 1) storeObservations <- data.frame(rexp(numberOfObservations, lambda))
  else
    storeObservations <- cbind(storeObservations, rexp(numberOfObservations, lambda))
    means = c(means, mean(storeObservations[,i]))
}

# Draw results
hist(means, main="Experiment Sample Means", xlab="Mean", ylab="Mean Frequency", col="light blue")
abline(v=mean(means), col="yellow", lwd=2)
abline(v=(1/lambda), col="red", lwd=2)
```

The code above generates 40 observations with the lambda rate we specified, takes the mean, stores it for later use, and finally runs this 1000 times. Since a requrement of the Central Limit Theorem is a very large data set, that is what the purpose of this code was: generate a large volume of predictable data. I have also collected the observational data for later discussions.

Now, our expected mean should be 5. And, from our data we see that the sample mean is very close to this: `r round(mean(means), 2)`. And, this is what our theorem predicts would occur.
I have added a vertical yellow line for the sample mean and a vertical red line for the theoretical values. In fact, you may not even be able to see the second line.

## Exploring Point [2] | The Variance
```{r}
varStore <- 0
for(i in 1:1000){
  varStore[i] <- var(storeObservations[,i])
}

hist(varStore, main="Experiment Sample Variance", xlab="Variance", ylab="Variance Frequency", col="light blue")
abline(v=mean(varStore), col="yellow", lwd=1)
abline(v=(1/lambda)^2, col="red", lwd=1)
```


For our theoretical distribution, the Standard Deviation of the Exponential function is 1/lambda. Which, again, is 5 as was calculated earlier. Since the Variance of a distribution is the Square root of the population, it follows that we can square the population Standard Deviation to calculate this.
For comparision: Theoretical Variance `r (1/lambda)^2` Versus Mean Sample Variance `r round(mean(varStore),2)`


## Exploring Point [3] | The Normal Distribution

```{r}
par(mfrow = c(1,2))

hist(means, main="Experiment Sample Means", xlab="Mean", ylab="Mean Frequency", col="light blue")
hist(rnorm(1000000), main="Normal Distribution", xlab="Mean", ylab="Mean Frequency", col="light blue")
```

Shown above is a comparison of our Sample Mean and an Example Normal Distribution generated by R.
You can see from a comparision that both are similair to one another. Normal distributions generate a mound shaped histogram where the mean value is at the center and as you travel outwards, you find that the number of values decreases continually until tehre are no more values.
